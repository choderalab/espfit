{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8425a3a5",
   "metadata": {},
   "source": [
    "# Mock code for preparing and loading data for training espaloma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import espaloma\n",
    "import espfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f1027",
   "metadata": {},
   "source": [
    "## Download QC datasets from QCArchive as HDF5 (SKIP IMPLEMENTATION) \n",
    "\n",
    "This functionality will not be implemented at the moment and alternatively rely on external scripts (e.g. https://github.com/choderalab/download-qca-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69087c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holder\n",
    "\n",
    "outdir='/DATASET_HDF_PATH/MYDATA' \n",
    "outfile='small_basic.hdf5'\n",
    "\n",
    "espfit.utils.data.download_qcarchive(workflow='Datataset', \n",
    "                                     qc_specification='default', \n",
    "                                     outdir=outdir,\n",
    "                                     outfile=outfile\n",
    "                                     )\n",
    "#> raise NotImplemented Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae11b77",
   "metadata": {},
   "source": [
    "## Convert HDF5 to DGL graphs (SKIP IMPLEMENTATION)\n",
    "\n",
    "This function will not be implemented at the moment and alternatively rely on external scripts (e.g. https://github.com/choderalab/refit-espaloma/blob/main/openff-default/01-create-dataset/script/getgraph_hdf5.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c152ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holder\n",
    "\n",
    "indir = '/DATASET_HDF_PATH/MYDATA'\n",
    "outdir = '/DATASET_DGL_PATH/MYDATA'\n",
    "\n",
    "_filenames = [ 'small_basic.hdf5', 'small_optimize.hdf5', 'small_torsiondrive.hdf5', 'peptide_basic.hdf5', 'peptide_optimize.hdf5', 'peptide_torsiondrive.hdf5' ]\n",
    "filenames = [ os.path.join(indir, filename) for filename in _filenames ]\n",
    "\n",
    "for filename in filenames:\n",
    "    ds += espfit.utils.data.hdf5_to_dgl(infile=filename,outdir=outdir)\n",
    "    \n",
    "#> raise NotImplemented Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f75f8",
   "metadata": {},
   "source": [
    "## Filter DGL graphs (SKIP IMPLEMENTATION)\n",
    "\n",
    "This function will not be implemented at the moment and rely on external scripts (e.g. https://github.com/choderalab/refit-espaloma/tree/main/openff-default/02-train/merge-data/script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f81b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holder\n",
    "\n",
    "outdir = '/DATASET_DGL_PATH/MYDATA/FILTERED'\n",
    "ds.filter(min_energy=0.1,\n",
    "          min_conformer=3,\n",
    "          compute_am1bcc='AM1BCC-ELF10', \n",
    "          compute_baseline_forcefields=forcefield_list, \n",
    "          compute_relative_energy=True,\n",
    "          subtract_nonbonded=True,\n",
    "          base_forcefiled='openff-2.0.0',\n",
    "          inplace=False,\n",
    "          outdir=outdir\n",
    "         )\n",
    "            \n",
    "#> raise NotImplemented Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da73a3",
   "metadata": {},
   "source": [
    "## Load preprocessed DGL graphs\n",
    "\n",
    "We are going to start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = '/DATASET_DGL_PATH/MYDATA/FILTERED/*'   # single path or list of paths\n",
    "ds = espfit.utils.data.load(in_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06051b0",
   "metadata": {},
   "source": [
    "#### Check properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.n_data   # number of data (entries)\n",
    "#> 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fca213",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.n_conf   # number of conformations\n",
    "#> 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd853ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.elements   # elements\n",
    "#> H,B,Br,C,N,O,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d118175",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.duplicate_isomeric_smiles   # isomeric smiles\n",
    "#> returns list of duplicate isomeric smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.duplicate_nonisomeric_smiles   # nonisomeric smiles\n",
    "#> returns list of duplicate nonisomeric smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af7103",
   "metadata": {},
   "source": [
    "#### Drop/merge duplicate smiles and filter datasets\n",
    "\n",
    "Ensure the datasets loaded from different sources have no duplicated smiles.  \n",
    "Drop duplicate isomeric (nonisomeric) smiles across different sources of datasets.  \n",
    "Merge duplicate dgl graphs with same smiles into a single dgl graph and create a new dataset called 'misc'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15340bae",
   "metadata": {},
   "source": [
    "##### drop and merge smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/DATASET_DGL_PATH/MYDATA'\n",
    "ds.drop_merge_nonisomeric_smiles(outdir=outdir, outname='misc')   # miscellaneous\n",
    "\n",
    "# Alteratively,\n",
    "ds.drop_merge_isomeric_smiles(outdir=outdir, outname='misc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84aa3d",
   "metadata": {},
   "source": [
    "##### filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958112bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add misc dataset that was just created\n",
    "ds += espfit.utils.data.load('/DATASET_DGL_PATH/MYDATA/misc')\n",
    "\n",
    "# Filter all dataset\n",
    "outdir = '/DATASET_DGL_PATH/MYDATA/FILTERED'\n",
    "ds.filter(min_energy=0.1,\n",
    "          min_conformer=3,\n",
    "          inplace=False,\n",
    "          outdir=outdir\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c7c7c-bd74-4009-a748-b014a9e21e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all dataset\n",
    "ds.compute(compute_am1bcc=None, \n",
    "           compute_baseline_forcefields=None, \n",
    "           compute_relative_energy=True,\n",
    "           subtract_nonbonded=True,\n",
    "           base_forcefiled='openff-2.0.0',\n",
    "           inplace=False,\n",
    "           outdir=outdir\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96faf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we could just filter the misc data and reload all filtered dataset later\n",
    "\n",
    "outdir = '/DATASET_DGL_PATH/MYDATA/FILTERED'\n",
    "misc_data = espfit.utils.data.load('/DATASET_DGL_PATH/MYDATA/misc')\n",
    "misc_data.filter(min_energy=0.1,\n",
    "                 min_conformer=3,\n",
    "                 inplace=False,\n",
    "                 outdir=outdir\n",
    "                 )\n",
    "misc_data.compute(compute_am1bcc=None, \n",
    "                  compute_baseline_forcefields=None, \n",
    "                  compute_relative_energy=True,\n",
    "                  subtract_nonbonded=True,\n",
    "                  base_forcefiled='openff-2.0.0',\n",
    "                  inplace=False,\n",
    "                  outdir=outdir\n",
    "                  )\n",
    "\n",
    "# load filtered\n",
    "input_dirs = glob.glob('/DATASET_DGL_PATH/MYDATA/FILTERED/*')   # list of paths\n",
    "ds = espfit.utils.data.load(input_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132c68d",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174a95f",
   "metadata": {},
   "source": [
    "#### Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b64d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2666\n",
    "ds.shuffle(RANDOM_SEED)\n",
    "\n",
    "ds_tr, ds_vl_te = ds.split(0.8, 0.2)\n",
    "ds_vl, ds_te = ds_vl_te.split(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de6513",
   "metadata": {},
   "source": [
    "#### Augment conformations to handle heterographs\n",
    "\n",
    "This is a work around to handle different graph size (shape). DGL requires at least one dimension with same size. \n",
    "Here, we will modify the graphs so that each graph has the same number of conformations instead fo concatenating \n",
    "graphs into heterogenous graphs with the same number of conformations. This will allow batching and shuffling \n",
    "during the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary data from graph in backend? (will this speed up training?)\n",
    "# e.g. g.nodes['g'].data.pop('u_qm')\n",
    "\n",
    "outdir = '/DATASET_DGL_PATH/MYDATA/FILTERED/RESHAPE'\n",
    "ds_tr.reshape(n_conf=50,\n",
    "              preserve_min=True,\n",
    "              inplace=True,\n",
    "              outdir=outdir,\n",
    "              verbose=1,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9deeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regenerate impropers (forgot why we need to do this)\n",
    "ds_tr.apply(regenerate_impropers, in_place=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45feb3f",
   "metadata": {},
   "source": [
    "## Train espaloma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "model = espfit.app.experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define espaloma architecture\n",
    "\n",
    "# use toml\n",
    "import yaml\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)    \n",
    "    \n",
    "# Possible methods\n",
    "# 1. call predefined model?\n",
    "model.call(model_name='model1')\n",
    "# 2. create model using yaml config\n",
    "model.create(config=config)\n",
    "# 3. from file\n",
    "model.from_file('config.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32760aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check neural network model\n",
    "\n",
    "model.net\n",
    "#> returns neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "model.train_data = ds_tr\n",
    "model.validation_data   = ds_vl\n",
    "model.test_data  = ds_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeeb626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data property\n",
    "\n",
    "model.train_data.n_data\n",
    "model.train_data.n_conf\n",
    "model.train_data.elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20caa2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoint file to `checkpoints` every 10 epochs\n",
    "# restart training from checkpoint file\n",
    "# validation is excluded from the training to decrease inference time\n",
    "\n",
    "model.train(steps, lr, batch_size, restart=checkpoint, checkpoint_frequency=10, log_file=logfile, log_level='debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25f33e",
   "metadata": {},
   "source": [
    "#### Validate and find best model\n",
    "\n",
    "Use job array to speed up this process using external scripts (e.g. https://github.com/choderalab/refit-espaloma/tree/main/openff-default/02-train/joint-improper-charge/charge-weight-1.0/eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b4be6",
   "metadata": {},
   "source": [
    "## Alternatively, train and validate simultaneously\n",
    "\n",
    "Not sure how slower this will be compared to just doing trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc585ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_val(steps, lr, batch_size, restart=checkpoint, checkpoint_frequency=10, logfile=logfile, verbose=1, early_stopping=800, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80498680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save()   # saves best model as 'model.pt'\n",
    "\n",
    "# plot loss validation\n",
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245006e3",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861df48",
   "metadata": {},
   "source": [
    "#### RMSE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff42837",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2666\n",
    "indir='/DATASET_DGL_PATH/MYDATA/FILTERED/RESHAPE'\n",
    "data_split_size = [0.8, 0.1, 0.1]\n",
    "best_model = 'model.pt'\n",
    "\n",
    "df = espfit.utils.rmse_metric(best_model, indir, data_split_size, RANDOM_SEED)   # pandas dataframe\n",
    "df.to_csv('rmse_metric.csv', index=False, sep='Â¥t', float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75ac51",
   "metadata": {},
   "source": [
    "#### Run other benchmarks independantly.\n",
    "\n",
    "- Small molecule geometry optmization (https://github.com/choderalab/geometry-benchmark-espaloma/tree/main/qc-opt-geo)\n",
    "- ESP benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ad7fb",
   "metadata": {},
   "source": [
    "## Train espaloma with experimental observable refitting\n",
    "\n",
    "- `espfit_experiment/`\n",
    "    - `data/`: Cached dataset ready for training\n",
    "    - `utils/`: Stores scripts to run external benchmarks\n",
    "        - `small_molecule_geometry`\n",
    "            - geo.py\n",
    "        - `partial_charge_esp`\n",
    "            - ele.py\n",
    "        - `rna_nucleoside`\n",
    "            - rna_nucleoside.py\n",
    "        - `rna_tetramer`:\n",
    "            - rna_tetramer.py\n",
    "    - `experiment/`\n",
    "        - `001/`: Create new directory for each refitting experiment\n",
    "            - `xml/`: Stores openmm xml\n",
    "            - `refit/`: Espaloma training\n",
    "                - `checkpoints/`: Stores checkpoint files\n",
    "                - `sampling/`: MD simulation\n",
    "                    - `iter_0`: Initial MD sampling\n",
    "                    - `iter_n`: MD sampling at epoch-n when necesssary\n",
    "                - `train.log`: Log file during espaloma training\n",
    "            - `benchmark/`\n",
    "                - `rmse_metric`\n",
    "                - `small_molecule_geometry`\n",
    "                - `partial_charge_esp`\n",
    "                - `rna_nucleoside`\n",
    "                - `rna_tetramer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67893219",
   "metadata": {},
   "source": [
    "#### Basic usage to run simulations for registered systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check registered systems\n",
    "registered_systems = espfit.system.available()\n",
    "\n",
    "registered_systems.get_names\n",
    "#> ['A', 'G', 'C', 'U', 'ApA']\n",
    "\n",
    "registered_systems.get('name').observables\n",
    "#> returns pandas dataframe with all experimental obervables and corresponding literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5daf12a",
   "metadata": {},
   "source": [
    "##### Prepare system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = registered_systems.get('name')\n",
    "simulation = system.setup(system_name=name, espaloma_model = 'model.pt', config=config, outdir=outdir)   # save xml\n",
    "\n",
    "# minimize\n",
    "simulation.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d56fba",
   "metadata": {},
   "source": [
    "##### Load a system already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = espfit.system.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc790e6",
   "metadata": {},
   "source": [
    "##### Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1528dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation.run(steps=100)   # standard MD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77fe98",
   "metadata": {},
   "source": [
    "##### Compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0657b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_exp = system.get_experimental_value()\n",
    "obs_calc = simulation.compute_observable()\n",
    "loss = simulation.compute_loss(obs_exp, obs_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358db78",
   "metadata": {},
   "source": [
    "##### Reweight observable using updated espaloma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18099bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = simulation.compute_reweighted_observable(update_espaloma_model='new.pt')\n",
    "\n",
    "# reweighted observable\n",
    "obs_calc = result.observable\n",
    "\n",
    "# effective sample size\n",
    "n_eff = result.effective_sample_size\n",
    "\n",
    "# loss with reweighted observable\n",
    "loss = simulation.compute_loss(obs_exp, obs_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98996eed",
   "metadata": {},
   "source": [
    "## Pseudo code for training espaloma with reweighting on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73acfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2666\n",
    "\n",
    "input_dirs = glob.glob('/DATASET_DGL_PATH/MYDATA/FILTERED/RESHAPE/*')   # list of paths\n",
    "ds = espfit.utils.data.load(input_dirs)\n",
    "ds.shuffle(RANDOM_SEED)\n",
    "\n",
    "ds_tr, ds_vl_te = ds.split(0.8, 0.2)\n",
    "ds_vl, ds_te = ds_vl_te.split(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e48171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = espfit.app.experiment()\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)    \n",
    "model.create(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a835423",
   "metadata": {},
   "source": [
    "##### Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = registered_systems.get('A')\n",
    "simulation = system.setup(system_name=name, espaloma_model = 'model.pt', config=config, outdir=outdir)   # save xml\n",
    "simulation.min()\n",
    "simulation.run(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fcd47",
   "metadata": {},
   "source": [
    "##### Get experimental observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_exp = system.get_experimental_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915bb00",
   "metadata": {},
   "source": [
    "##### Train with MD reweighting\n",
    "\n",
    "[Iterative Optimization of Molecular Mechanics Force Fields from NMR Data of Full-Length Proteins, JCTC, 2011](https://pubs.acs.org/doi/full/10.1021/ct200094b)  \n",
    "[Automatic Learning of Hydrogen-Bond Fixes in the AMBER RNA Force Field, JCTC, 2022](https://pubs.acs.org/doi/10.1021/acs.jctc.2c00200)  \n",
    "[Enhanced sampling methods for molecular dynamics simulations, arXiv, 2022](https://arxiv.org/abs/2202.04164)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tr_loader = dgl.dataloading.GraphDataLoader(ds_tr, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.net().parameters(), lr=learning_rate)\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for idx in range(steps):\n",
    "        n_eff = []   # store effective sample size\n",
    "        for g in ds_tr_loader:\n",
    "            optimizer.zero_grad()\n",
    "            g = g.to(\"cuda:0\")\n",
    "            g.nodes[\"n1\"].data[\"xyz\"].requires_grad = True \n",
    "            \n",
    "            # Original espaloma loss\n",
    "            loss = net(g)\n",
    "\n",
    "            # Reweighting \n",
    "            result = simulation.compute_reweighted_observable(net)   # return: (reweighted observable, effective sample size)\n",
    "            obs_calc = result.observable\n",
    "            loss_md = simulation.compute_loss(obs_exp, obs_calc)   \n",
    "            \n",
    "            n_eff += result.n_eff\n",
    "            \n",
    "            # Joint loss\n",
    "            loss += weight * loss_md\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # save checkpoint file \n",
    "            if idx % 10 == 0:\n",
    "                if not os.path.exists(output_prefix):\n",
    "                    os.mkdir(output_prefix)\n",
    "                torch.save(net.state_dict(), output_prefix + \"/net%s.pth\" % idx)\n",
    "                \n",
    "        # Averaged effective samples\n",
    "        if n_eff.mean() < effective_sample_size_tolerance:\n",
    "            # rebuild system with current net model\n",
    "            # rerun simulation\n",
    "            # cache new trajectory\n",
    "            simulation.rebuild()\n",
    "            simulation.run()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3900fc9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5735fb72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ec7b85df",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
